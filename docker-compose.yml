services:
  model:
    image: "inference-api:latest"

  web:
    image: "web-frontend"
    ports:
     - "8080:8080"
    environment:
     - MODEL_HOST=http://model:8080